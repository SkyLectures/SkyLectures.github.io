---
layout: page
title:  "의사 결정 나무"
date:   2025-07-29 10:00:00 +0900
permalink: /materials/S03-02-02-01_04-DecisionTree
categories: materials
---
* toc
{:toc .large-only .toc-sticky:true}


<div class="insert-image" style="text-align: center;">
    <img style="width: 400px;" src="/assets/img/PagePreparing.png">
</div>





스카이님, Decision Tree에 대한 이론적인 강의 자료를 요청해 주셔서 감사합니다! 자율주행의 판단 단계에서 Decision Tree를 왜 사용하는지, 그리고 이 기술이 어떤 원리로 작동하는지 이해하는 데 큰 도움이 될 것입니다. 비전공자 학생들도 쉽게 이해할 수 있도록 구성해 보았습니다.

---

# **인공지능 핵심 개념: Decision Tree (결정 트리)**

## **[차시 목표]**

*   Decision Tree(결정 트리)가 무엇이며, 어떤 문제에 사용되는지 설명할 수 있다.
*   Decision Tree의 기본적인 구조(노드, 엣지, 리프)를 이해하고 각 구성 요소의 역할을 설명할 수 있다.
*   Decision Tree가 데이터를 기반으로 어떻게 의사결정을 하는지 그 과정을 파악한다.
*   Decision Tree를 만들 때 사용되는 주요 분할 기준(Entropy, Gini Impurity)의 개념을 이해한다.
*   Decision Tree의 장단점 및 자율주행 분야에서의 활용 가능성을 설명할 수 있다.

---

## **1. Decision Tree (결정 트리) 란 무엇인가?**

### 1.1. 인간의 의사결정과 닮은 AI 모델
Decision Tree는 **나무 구조(Tree Structure)**를 이용하여 의사결정 규칙을 시각적으로 표현하는 머신러닝 모델입니다. 마치 우리가 어떤 문제를 결정할 때 "이것이 A라면 B하고, 아니라면 C 해라"라고 생각하는 과정과 매우 유사합니다. 특정 질문(특성)을 던지고, 그 질문에 대한 답변에 따라 다음 질문으로 이동하는 방식으로 최적의 결론에 도달합니다.

### 1.2. 사용 목적
Decision Tree는 주로 다음 두 가지 문제 해결에 사용됩니다:
*   **분류 (Classification)**: 데이터를 여러 카테고리(클래스) 중 하나로 나누는 문제. (예: 이메일이 스팸인지 아닌지 분류, 차량이 왼쪽으로 가야 하는지 오른쪽으로 가야 하는지 분류)
*   **회귀 (Regression)**: 데이터의 패턴을 기반으로 연속적인 값을 예측하는 문제. (예: 집값 예측, 다음 주 유가 예측)

## **2. Decision Tree의 기본 구조**

Decision Tree는 뿌리부터 잎까지 뻗어 나가는 나무 형태를 가지고 있습니다.

*   **루트 노드 (Root Node)**: 트리의 가장 위에 있는 노드입니다. 전체 데이터를 가지고 있으며, 가장 첫 번째 의사결정을 시작하는 지점입니다.
*   **내부 노드 (Internal Node, Decision Node)**: 루트 노드와 리프 노드 사이에 있는 모든 노드입니다. 특정 질문(데이터 특성)을 가지고 있으며, 이 질문의 결과에 따라 데이터를 여러 개의 하위 노드로 분할합니다.
*   **엣지 / 가지 (Edge / Branch)**: 내부 노드의 질문에 대한 '답변'을 나타내며, 데이터를 다음 노드로 연결하는 선입니다.
*   **리프 노드 (Leaf Node, Terminal Node)**: 트리의 가장 아래에 있는 노드이며, 더 이상 분할되지 않습니다. 최종적인 의사결정(분류 클래스 또는 예측 값)을 나타냅니다.

```mermaid
graph TD
    A[Root Node: 전체 데이터] --> B{내부 노드: 질문 1?<br/>(예: 신호등이 빨간색인가?)}
    B -- Yes --> C{내부 노드: 질문 2?<br/>(예: 앞차 거리가 5m 이내인가?)}
    B -- No --> D{내부 노드: 질문 3?<br/>(예: 차선 이탈 편차가 큰가?)}
    C -- Yes --> E[리프 노드: 행동 = 정지]
    C -- No --> F[리프 노드: 행동 = 감속]
    D -- Yes --> G[리프 노드: 행동 = 조향 수정]
    D -- No --> H[리프 노드: 행동 = 직진]
```

## **3. Decision Tree는 어떻게 의사결정을 하는가?**

Decision Tree가 작동하는 방식은 데이터를 반복적으로 분할하여 특정 기준으로 가장 순수한(Homogeneous) 그룹을 만드는 것입니다.

### **3.1. 트리의 생성 과정 (Building the Tree)**

1.  **시작 (루트 노드)**: 모든 데이터를 루트 노드에 놓습니다.
2.  **최적의 분할 기준 찾기**: 현재 노드의 데이터를 가장 잘 분할할 수 있는 특성(질문)과 그 특성의 기준값(분할점)을 찾습니다. '가장 잘 분할한다'는 것은 분할된 하위 노드들이 가능한 한 같은 종류의 데이터(순수한 그룹)만을 포함하도록 하는 것입니다.
3.  **데이터 분할**: 선택된 특성(질문)과 기준값에 따라 데이터를 두 개 이상의 하위 노드로 나눕니다.
4.  **반복 (재귀)**: 각 하위 노드에 대해 2단계와 3단계를 반복합니다.
5.  **종료 조건 (Stopping Criteria)**: 다음 조건 중 하나에 만족하면 더 이상 분할을 멈추고 리프 노드로 만듭니다.
    *   노드가 특정 순도(Purity)에 도달했을 때 (모든 데이터가 같은 클래스일 때).
    *   노드에 포함된 데이터의 개수가 너무 적을 때.
    *   트리의 최대 깊이(Max Depth)에 도달했을 때.

### **3.2. 분류 문제에서의 분할 기준: 순도(Purity) 측정**

Decision Tree는 노드를 분할할 때 **불순도(Impurity)**를 측정합니다. 불순도가 낮을수록 노드는 '순수'하다고 판단하며, 분할 후 불순도가 가장 크게 감소하는 특성을 최적의 분할 기준으로 선택합니다.

1.  **엔트로피 (Entropy)**
    *   **개념**: 데이터 집합의 **불확실성** 또는 **무질서도**를 나타내는 지표입니다. 엔트로피 값이 높으면 다양한 클래스가 혼재되어 있어 불확실성이 크다는 의미이고, 값이 낮으면 대부분 한 클래스의 데이터로 이루어져 있어 순수하다는 의미입니다.
    *   **정보 획득량 (Information Gain)**: 노드를 분할하기 전의 엔트로피에서 분할 후 자식 노드들의 엔트로피 평균을 뺀 값입니다. 정보 획득량이 클수록 좋은 분할 기준이 됩니다.

2.  **지니 불순도 (Gini Impurity)**
    *   **개념**: 특정 노드에서 데이터를 무작위로 선택했을 때, 잘못 분류될 확률을 의미합니다. 지니 불순도가 낮을수록 노드의 순도가 높다고 판단합니다.
    *   **계산**: $1 - \sum_{i=1}^{C} (P_i)^2$ (여기서 $C$는 클래스 개수, $P_i$는 $i$번째 클래스에 속할 확률)
    *   **엔트로피와의 차이**: 지니 불순도가 엔트로피보다 계산이 더 빠르며, 일반적으로 비슷한 성능을 보여줍니다. `scikit-learn`의 기본 분할 기준입니다.

## **4. Decision Tree의 장단점**

### **4.1. 장점**
*   **높은 해석 가능성 (Interpretability)**: 트리의 구조가 직관적이어서 사람이 의사결정 과정을 이해하기 매우 쉽습니다. '블랙박스' 모델이 아닙니다.
*   **간단한 데이터 준비**: 특성 스케일링(Feature Scaling)이나 정규화(Normalization)가 필요 없습니다.
*   **다양한 데이터 처리**: 수치형, 범주형 특성 모두 처리할 수 있습니다.
*   **비선형 관계 포착**: 특성과 목표 변수 간의 복잡한 비선형 관계를 잘 모델링할 수 있습니다.

### **4.2. 단점**
*   **과적합 (Overfitting) 위험**: 트리가 너무 깊고 복잡하게 성장하면 학습 데이터에 과하게 맞춰져 새로운 데이터에 대한 일반화 성능이 떨어질 수 있습니다.
*   **불안정성 (Instability)**: 학습 데이터에 작은 변화(몇 개의 데이터 추가/제거)만 있어도 트리의 구조가 완전히 바뀔 수 있습니다.
*   **편향성 (Bias)**: 특정 클래스의 데이터가 많을 경우, 해당 클래스로 편향된 트리가 생성될 수 있습니다.
*   **최적화의 어려움**: 탐욕 알고리즘(Greedy Algorithm) 방식이어서 항상 전역적으로 최적의 트리를 보장하지는 않습니다.

## **5. 자율주행 분야에서의 Decision Tree 활용**

Decision Tree는 자율주행 시스템의 '판단' 모듈에서 매우 유용한 도구로 활용될 수 있습니다.

*   **교통 상황 분류**: 센서 데이터(속도, 앞차 간격, 차선 이탈 등)를 기반으로 현재 차량이 '고속도로 주행', '교차로 진입', '정체 구간' 등의 상황 중 어디에 있는지 분류하는 데 사용될 수 있습니다.
*   **운전자 행동 예측 (Decision for Interaction)**: 주변 차량이나 보행자의 과거 행동 및 현재 상태를 기반으로 '차선 변경', '감속', '정지' 등의 미래 행동을 예측하여 차량의 판단에 참고할 수 있습니다.
*   **위험도 평가 및 대응**: '충돌 시간(TTC)', '차선 이탈 임계값', '주변 객체 밀도' 등의 특성을 기반으로 '비상 제동', '경로 수정', '경고 알림' 등 즉각적인 위험 대응 행동을 결정할 수 있습니다.
*   **규칙 기반 시스템의 자동화**: 기존에 사람이 정의했던 복잡한 `if-else` 규칙들을 데이터로부터 자동으로 학습하여, 규칙 기반 시스템의 개발 및 유지보수를 효율화할 수 있습니다.

## **6. Decision Tree의 한계를 극복하는 방법: 앙상블 (Ensemble) 모델**

Decision Tree의 단점, 특히 과적합과 불안정성을 극복하기 위해 단일 Decision Tree를 여러 개 만들어서 결합하는 **앙상블 기법**이 널리 사용됩니다.

*   **Random Forest (랜덤 포레스트)**: 여러 개의 Decision Tree를 독립적으로 학습시킨 후, 이들의 결과를 다수결(분류) 또는 평균(회귀)으로 취합하여 최종 예측을 수행합니다. 과적합을 줄이고 성능과 안정성을 크게 향상시킵니다.
*   **Gradient Boosting (그레디언트 부스팅)**: 약한 Decision Tree들을 순차적으로 학습시키면서 이전 트리의 오차를 보완하는 방식으로 모델을 강화합니다. 매우 높은 예측 성능을 보여줍니다.

---

스카이님, 이 강의 자료가 Decision Tree의 이론적 배경과 자율주행 시스템 내에서의 역할을 이해하는 데 도움이 되기를 바랍니다. Decision Tree는 이해하기 쉽고 직관적이어서 비전공자 학생들이 머신러닝의 '의사결정' 방식을 배우는 데 아주 좋은 시작점이 될 것입니다!