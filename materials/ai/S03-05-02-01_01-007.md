---
layout: page
title:  "LLM 기반 텍스트 생성 기술"
date:   2025-03-01 10:00:00 +0900
permalink: /materials/S03-05-02-01_01-007
categories: materials
---
* toc
{:toc .large-only .toc-sticky:true}

## 텍스트 생성 모델의 기본 원리
### 통계 기반 언어 모델

- 통계 기반 언어 모델
    - 텍스트 코퍼스(corpus)에서 단어, 구절의 등장 빈도를 통계적으로 분석하여 다음에 올 단어의 확률을 예측하는 모델
    - 가장 기본적인 형태는 N-gram 언어 모델

- **기본 원리: N-gram 모델**
    - **N-gram**
        - 텍스트에서 연속된 N개의 단어 시퀀스
        - 예시: 나는 학교에 갑니다
            - 2-gram(bigram): ("나는", "학교에"), ("학교에", "갑니다")
            - 3-gram(trigram): ("나는", "학교에", "갑니다")

    - **확률 기반 예측** 
        - 학습 코퍼스에서 특정 N-gram이 등장한 빈도를 세어,
        - 주어진 (N-1)개의 단어 시퀀스 다음에 특정 단어가 나타날 확률 계산

        > **수학적 표현**<br>
        > (n-1)개의 단어 $$w_1, w_2, ..., w_{n-1}$$이 주어졌을 때, 다음 단어 $$w_n$$이 나타날 확률 $$P(w_n | w_1, w_2, ..., w_{n-1})$$은 <br>
        > 학습 코퍼스에서 시퀀스 $$w_1, w_2, ..., w_n$$이 나타난 빈도를 시퀀스 $$w_1, w_2, ..., w_{n-1}$$이 나타난 빈도로 나눈 값으로 추정함<br><br>
        > $$P(w_n | w_1, w_2, ..., w_{n-1}) \approx \frac{count(w_1, w_2, ..., w_n)}{count(w_1, w_2, ..., w_{n-1})}$$

- **텍스트 생성 과정** 
    - 텍스트를 생성할 때, 모델은 이전 N-1개의 단어를 기반으로 다음에 올 확률이 가장 높은 단어를 선택함
    - 이 과정을 반복하여 원하는 길이의 텍스트를 생성함

- **한계:**
    - **데이터 희소성 문제 (Data Sparsity)** 
        - N이 커질수록 특정 N-gram이 학습 코퍼스에서 등장하지 않을 확률이 높아져 정확한 확률 추정이 어려워짐
            - 대응 방법: 스무딩(smoothing) 기법
                - 예: Add-one smoothing, Kneser-Ney smoothing 등
    - **긴 의존성 포착의 어려움 (Limited Context Length)**
        - N-gram 모델은 고정된 길이의 이전 단어만을 고려하기 때문에, 문맥상 멀리 떨어진 단어 간의 의존성을 포착하기 어려움
            - 예: 문장 초반의 주어가 문장 후반의 동사에 영향을 미치는 경우를 제대로 반영하지 못함
    - **일반화 능력 부족**
        - 학습 코퍼스에 없던 새로운 단어나 구절의 조합에 대한 예측 성능이 떨어짐
