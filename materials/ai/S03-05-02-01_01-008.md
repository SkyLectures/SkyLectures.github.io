---
layout: page
title:  "LLM 기반 텍스트 생성 기술"
date:   2025-03-01 10:00:00 +0900
permalink: /materials/S03-05-02-01_01-008
categories: materials
---
* toc
{:toc .large-only .toc-sticky:true}

## 텍스트 생성 모델의 기본 원리
### 신경망 기반 언어 모델 (RNN, Transformer 등)

- 인공 신경망의 능력을 활용하여 텍스트의 복잡한 패턴과 장기 의존성을 학습하고, 이를 기반으로 다음 단어를 예측하는 모델
- 자연어 처리에서의 대표적인 신경망 구조: RNN(Recurrent Neural Network)과 Transformer 등

#### 1. 순환 신경망 (Recurrent Neural Network, RNN)

##### 1.1 기본 원리
- 순환적인 구조를 통해 이전 시점의 정보를 현재 시점의 처리에 반영할 수 있도록 설계됨
- 각 시점마다 입력 단어와 이전 시점의 은닉 상태(hidden state)를 입력받아 현재 시점의 은닉 상태를 갱신하고, 
- 다음 단어를 예측

> **수학적 표현**<br>
> $$t$$ 시점의 입력 단어 $$x_t$$, 이전 시점의 은닉 상태 $$h_{t-1}$$ 사용 → 현재 시점의 은닉 상태 $h_t$ 계산<br>
> 이를 바탕으로 다음 단어 $$y_t$$에 대한 확률 분포 출력<br><br>
> $$h_t = f(W_x x_t + W_h h_{t-1} + b_h)$$ <br>
> $$o_t = g(W_o h_t + b_o)$$ <br>
> $$P(y_t | x_1, ..., x_t) = softmax(o_t)$$ <br><br>
> - $$f$$: 활성화 함수(예: tanh)<br>
> - $$g$$: 출력 활성화 함수<br>
> - $$W_x, W_h, W_o$$: 가중치 행렬<br>
> - $$b_h, b_o$$: 편향벡터

##### 1.2 텍스트 생성 과정
- 초기 은닉 상태와 시작 토큰을 입력으로 하여 다음 단어를 예측
- 예측된 단어를 다음 시점의 입력으로 사용하여 텍스트 생성을 반복

##### 1.3 RNN의 종류 및 발전
- **LSTM (Long Short-Term Memory)** 
    - RNN의 고질적인 문제인 기울기 소실(vanishing)/폭발(exploding gradient) 문제를 해결하기 위해 제안된 구조
    - 기억 셀(cell state), 입력 게이트, 망각 게이트, 출력 게이트 도입
    - 장기 의존성의 효과적인 학습 지원

- **GRU (Gated Recurrent Unit)**
    - LSTM을 간소화한 구조
    - 업데이트 게이트, 리셋 게이트 사용
    - LSTM과 비슷한 성능 유지 + 파라미터 수 감소

##### 1.4 한계
- **긴 의존성 포착의 어려움 (여전히 존재)**
    - LSTM과 GRU가 어느 정도 장기 의존성 문제를 처리할 수 있지만, 
    - 매우 긴 시퀀스에서는 정보가 소실될 가능성이 여전히 존재
- **순차적인 처리 방식**
    - 입력을 순차적으로 처리해야 하므로 병렬 처리가 어렵고, 
    - 긴 시퀀스의 경우 학습 속도가 느림

#### 2. 트랜스포머 (Transformer) 아키텍처

##### 2.1 기본 원리
- 트랜스포머 아키텍처는 
    - 순환 구조를 사용하지 않고, 
    - **어텐션(Attention)** 모델을 이용하여
        - 입력 시퀀스 내의 모든 단어 간의 관계를 병렬적으로 파악
    - **셀프 어텐션(Self-Attention)** 모델을 통해
        - 문장 내 각 단어가 다른 단어와 얼마나 관련이 있는지 가중치를 계산
        - 이를 기반으로 문맥 정보를 효과적으로 추출
    - 주로 **인코더(Encoder)**와 **디코더(Decoder)** 구조로 구성됨
    - 언어 모델링, 특히 텍스트 생성에서는 주로 디코더 구조만을 사용하거나, 인코더-디코더 전체 구조를 활용하기도 함

- 모듈 별 기능
    - **셀프 어텐션**
        1. 입력 시퀀스의 각 단어에 대해 세 가지 다른 벡터(Query, Key, Value)를 생성
        2. Query와 Key 벡터 간의 유사도를 계산
        3. 각 단어가 다른 단어에 얼마나 집중해야 하는지를 결정
        4. 이 가중치를 Value 벡터에 곱하여 최종 어텐션 출력 도출

        > **수식 표현**<br><br>
        > $$Attention(Q, K, V) = softmax(\frac{QK^T}{\sqrt{d_k}})V$$<br><br>
        > - $$Q$$: Query 행렬
        > - $$K$$: Key 행렬
        > - $$V$$: Value 행렬
        > - $$d_k$$: Key 벡터의 차원

    - **포지셔널 인코딩 (Positional Encoding)**
        - 순환 구조가 없기 때문에 단어의 순서 정보를 명시적으로 모델에 제공하기 위해 사용
        - 각 단어의 위치에 따라 고유한 벡터를 더해줌

    - **멀티 헤드 어텐션 (Multi-Head Attention)** 
        - 여러 개의 독립적인 어텐션 헤드를 병렬로 수행
        - 다양한 관점에서 문맥 정보를 학습

    - **레이어 정규화 (Layer Normalization), 잔차 연결 (Residual Connection) 등** 
        - 모델의 학습 안정성과 성능 향상을 위해 사용되는 기법들

##### 2.2 텍스트 생성 과정 (디코더 기반)
- 이전까지 생성된 단어 시퀀스를 입력으로 받아, 
- 셀프 어텐션을 통해 문맥을 파악하고 
- 다음에 올 단어의 확률 분포를 예측
- 이 과정을 반복하여 텍스트를 생성

##### 2.3 장단점
- **장점**
    - **긴 의존성 포착 능력**
        - 어텐션 메커니즘을 통해 문장 내의 모든 단어 간의 관계를 직접적으로 파악하므로,
        - 장거리 의존성을 효과적으로 모델링할 수 있음
    - **병렬 처리 용이성**
        - 순환 구조가 아니므로
        - 입력 시퀀스의 모든 단어를 병렬로 처리할 수 있어
        - 학습 속도가 빠름
    - **높은 성능**
        - 다양한 자연어 처리 task에서 뛰어난 성능을 보여줌
        - 특히 대규모 언어 모델(LLM)의 기반 구조로 널리 사용됨

- **단점**
    - **계산 복잡도**
        - 입력 시퀀스 길이에 따라 어텐션 계산량이 제곱으로 증가하므로,
        - 매우 긴 시퀀스 처리에는 부담이 될 수 있음
    - **상대적인 순서 정보 부족 (포지셔널 인코딩으로 보완)**
        - 순환 구조가 아니므로 순서 정보를 명시적으로 주입해야 함