---
layout: page
title:  "LLM 기반 텍스트 생성 기술"
date:   2025-03-01 10:00:00 +0900
permalink: /materials/S03-05-02-01_01-011
categories: materials
---
* toc
{:toc .large-only .toc-sticky:true}

## LLM을 활용한 텍스트 생성 방식

### 샘플링 전략 및 디코딩 방식의 이해

- LLM이 다음 단어를 예측할 때 사용하는 샘플링 전략과 디코딩 방식은 생성되는 텍스트의 품질과 다양성에 큰 영향을 미침
- 다양한 전략을 이해하고 적용함으로써 원하는 특성의 텍스트를 생성할 수 있음

- **사용 예시**

    - Temperature 조절
        - 설명
            - Temperature: LLM이 다음 단어를 선택할 때의 무작위성을 조절하는 파라미터
                - 값이 높을수록
                    - 더 다양하고 창의적인 텍스트가 생성될 가능성이 있음
                    - 일관성이 떨어지거나 문법적으로 어색한 텍스트가 생성될 위험이 있음
                - 값이 낮을수록
                    - 더 결정론적이고 안전한 텍스트가 생성됨
                    - 창의성이 부족할 수 있음
        - 실습 (가상)
            - 동일한 프롬프트에 대해 Temperature 값을 0.2와 1.0으로 설정했을 때 생성되는 텍스트의 차이 비교(실제 LLM 인터페이스에서 설정 가능)

    - Top-k 샘플링
        - 설명
            - 모델이 예측한 다음 단어의 확률 분포에서 확률이 가장 높은 상위 k개의 단어 중에서만 다음 단어를 무작위로 선택하는 방식
            - 가능성이 낮은 단어의 선택을 방지하고 일관성을 유지하면서도 적절한 수준의 다양성을 확보할 수 있음
        - 실습 (가상)
            - Top-k 값을 5와 50으로 설정했을 때 생성되는 텍스트의 다양성 차이 비교(실제 LLM 인터페이스에서 설정 가능)

    - Top-p (Nucleus) 샘플링
        - 설명
            - 모델이 예측한 다음 단어의 확률을 누적하여 특정 임계값 p에 도달하는 단어 집합 내에서 다음 단어를 무작위로 선택하는 방식
            - Top-k와 유사하게 확률이 낮은 단어를 필터링하지만, 고정된 개수가 아닌 확률의 합을 기준으로 단어 집합을 결정하므로 문맥에 따라 선택될 수 있는 단어의 수가 달라짐
        - 실습 (가상)
            - Top-p 값을 0.5와 0.9로 설정했을 때 생성되는 텍스트의 예측 가능성과 창의성 사이의 균형 변화를 관찰 (실제 LLM 인터페이스에서 설정 가능)
