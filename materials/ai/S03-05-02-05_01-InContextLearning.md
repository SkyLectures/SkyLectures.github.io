---
layout: page
title:  "LLM 활용: 인 컨텍스트 러닝 맛보기"
date:   2025-03-01 10:00:00 +0900
permalink: /materials/S03-05-02-05_01-InContextLearning
categories: materials
---
* toc
{:toc .large-only .toc-sticky:true}

## 1. 인 컨텍스트 러닝이란?

- 사전 학습된 거대 언어 모델(LLM)이 명시적인 파인튜닝(Fine-tuning) 없이 주어진 프롬프트 내의 몇 가지 예시(demonstrations)만으로 새로운 작업을 수행하는 능력
- 이는 LLM의 놀라운 emergent ability 중 하나로, 모델이 학습 과정에서 명시적으로 특정 작업에 대해 훈련받지 않았음에도 불구하고 프롬프트에 제시된 문맥(context)만으로 작업을 이해하고 수행할 수 있음

### 1.1 인 컨텍스트 학습의 작동 방식

1. 프롬프트 구성
    - 사용자 질문이나 지시에 더불어, 모델이 수행해야 할 작업의 몇 가지 예시(입력-출력 쌍)를 함께 프롬프트에 포함함
    - 이 예시들은 모델이 원하는 작업 방식을 이해하도록 돕는 역할을 수행함

2. 명시적 파인튜닝 없음
    - 별도의 데이터셋으로 모델의 가중치를 업데이트하는 파인튜닝 과정 없이
    - 오직 주어진 프롬프트 내의 정보만을 활용하여 작업을 수행함

3. 패턴 매칭 및 일반화
    - LLM은 방대한 데이터 학습 과정에서 축적한 언어적 패턴과 일반적인 지식을 바탕으로
    - 프롬프트 내의 예시들의 패턴을 파악하고
    - 이를 새로운 입력에 적용하여
    - 적절한 출력을 생성함

4. 예시
    - 감성 분류 작업을 수행한다고 가정할 때,
        - 기존 방식이라면 감성 레이블이 붙은 많은 데이터를 사용하여 모델을 파인튜닝해야 함
        - 인 컨텍스트 학습에서는 다음과 같은 프롬프트를 LLM에 제공할 수 있음

            ```text
            다음 텍스트의 감성을 분류하세요 (긍정/부정):

            텍스트: 오늘 날씨가 정말 좋네요.
            감성: 긍정

            텍스트: 영화가 너무 지루했어요.
            감성: 부정

            텍스트: 이 식당 음식은 정말 훌륭해요!
            감성:
            ```

    - LLM은 프롬프트에 제시된 두 개의 예시를 통해 긍정적인 표현과 부정적인 표현을 학습하고, 
    - 마지막 텍스트 "이 식당 음식은 정말 훌륭해요!"에 대해 "긍정"이라는 답변을 생성할 수 있음

### 1.2 인 컨텍스트 학습의 주요 요소

- 프롬프트의 품질
    - 명확성: 수행해야 할 작업을 명확하게 지시하는 것이 중요함
    - 관련성: 제공되는 예시가 목표 작업과 관련성이 높아야 모델이 올바른 패턴을 학습할 수 있음
    - 일관성: 예시들의 형식과 내용이 일관성을 유지하는 것이 좋음

- 예시의 수 (Few-shot Learning)
    - 일반적으로 프롬프트에 제공되는 예시의 수에 따라 성능이 달라짐
        - Zero-shot Learning: 예시 없이 작업 지시만 제공하는 경우
        - One-shot Learning: 하나의 예시와 함께 작업 지시를 제공하는 경우
        - Few-shot Learning: 몇 개의 (일반적으로 3~10개 정도) 예시와 함께 작업 지시를 제공하는 경우
    - 예시 수가 증가할수록 성능이 향상되는 경향이 있지만
    - 일정 수준 이상에서는 효과가 미미하거나 오히려 성능 저하를 일으킬 수도 있음

- 예시의 순서
    - 예시의 순서가 모델의 성능에 영향을 미칠 수 있다는 연구 결과도 있음

- LLM의 크기 및 능력
    - 모델의 크기가 클수록,
    - 그리고 사전 학습 과정에서 다양한 패턴을 잘 학습한 모델일수록 
    - 인 컨텍스트 학습 능력이 뛰어남

> - <span style="color: red">**주의 사항**</span>
>   - <span style="color: red">프롬프트 작성 시, 너무 많은 정보와 상세한 지시를 제공하려고 하다가 LLM에게 특정 답변을 유도하는 형태로 프롬프트를 작성하게 되는 경우가 있음</span>
>   - <span style="color: red">"나는 이러이러하게 생각하는데 너는 어떻게 생각해?"와 같이 자신의 의도가 정보인 것처럼 전달될 때, LLM의 답변은 자신의 의도를 중심으로 답변을 구성하게 될 확률이 높음</span>
>   - <span style="color: red">따라서 프롬프트의 문장을 작성할 때 편향성을 띄지 않도록 주의할 필요가 있음</span>
> 

### 1.3 인 컨텍스트 학습의 장점

- 파인튜닝 불필요
    - 별도의 파인튜닝 데이터셋 구축 및 학습 과정이 필요 없으므로 시간과 자원을 절약할 수 있음

- 빠른 적용
    - 새로운 작업에 대한 프롬프트만 구성하면 되기때문에 빠르게 다양한 작업에 LLM을 적용할 수 있음

- 유연성
    - 다양한 작업에 대해 프롬프트만 변경하여 적용할 수 있어 모델의 활용도가 높음

- 데이터 부족 문제 완화
    - 파인튜닝에 필요한 대규모 데이터셋이 없는 경우에도 LLM의 일반적인 지식을 활용하여 작업을 수행할 수 있음

### 1.4 인 컨텍스트 학습의 한계

- 프롬프트 설계의 어려움
    - 효과적인 프롬프트를 설계하는 것은 여전히 어려운 과제이며, 
    - 시행착오를 거쳐야 할 수 있음

- 성능 제한
    - 파인튜닝된 모델에 비해 성능이 낮을 수 있음
    - 특히 복잡하거나 특정한 지식을 요구하는 작업에서는 한계가 드러날 수 있음

- 문맥 길이 제한
    - LLM의 입력 문맥 길이에는 제한이 있으므로
    - 너무 많은 예시를 포함하거나 긴 프롬프트를 사용하는 데 어려움이 있을 수 있음

- 비용
    - 긴 프롬프트는 토큰 사용량을 증가시켜 API 사용 비용이 높아질 수 있음

- 설명 가능성 부족
    - 모델이 어떤 근거로 특정 출력을 생성했는지 명확하게 설명하기 어려울 수 있음

### 1.5 인 컨텍스트 학습의 중요성 및 미래

- 인간-AI 상호작용 방식의 변화
    - 사용자가 자연어 프롬프트만으로 LLM과 상호작용하며 원하는 작업을 수행할 수 있는 새로운 가능성을 제시

- LLM 활용의 용이성 증대
    - 전문적인 지식 없이도 LLM을 다양한 애플리케이션에 쉽게 통합하고 활용할 수 있도록 함

- 지속적인 연구 분야
    - 인 컨텍스트 학습 능력을 향상시키고 그 원리를 이해하기 위한 활발한 연구가 진행되고 있음
    - 향후 연구를 통해 
        - 더욱 효과적인 프롬프트 설계 방법론
        - 더 적은 수의 예시로 높은 성능을 달성하는 기술
        - 그리고 인 컨텍스트 학습의 근본적인 작동 원리에 대한 이해가 깊어질 것으로 기대됨

## 2. 실습

### 2.1 인텍스트 러닝 알아보기

- '지피티'로 삼행시를 써 줘
- '지피티'로 삼행시를 쓴다는 것은 각 행을 '지', '피', '티' 글자로 시작한다는 거야. 다시 작성해봐. (GPT-4는 가르쳐주지 않아도 삼행시를 쓸 수 있음)

### 2.2 수학문제 풀이 퀴즈

- 8 # 2는 8을 2로 두 번 나누는 것으로 정의하자. 즉 8 # 2 = 2야. 그럼 27 # 3은 얼마야?
- 다음과 같은 규칙이 있을 때 다음에 올 숫자는 뭐야? 2, 6, 18, 54

### 2.3 국가별 수도 맞추기 퀴즈

- 다음과 같은 규칙이 있을 때 '네팔'에 대한 답은 뭘까?
대한민국:서울
미국: 워싱턴
프랑스: 파리

