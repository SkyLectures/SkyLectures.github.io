---
layout: page
title:  "Ollama 개요"
date:   2025-03-01 10:00:00 +0900
permalink: /materials/S03-05-06-01_01-OllamaOverview
categories: materials
---
* toc
{:toc .large-only .toc-sticky:true}


## 1. Ollama란?
- 대규모 언어 모델(Large Language Model, LLM)을 로컬 환경에서 실행, 관리할 수 있도록 설계된 오픈소스 플랫폼
- 개인 사용자부터 개발자까지 누구나 쉽게 LLM을 활용할 수 있도록 설계된 강력한 도구
- 클라우드 기반 API에 의존하지 않고 자신의 PC에서 직접 LLM을 실행할 수 있음
- 복잡한 설정 없이 간단한 명령만으로 다양한 모델을 다운로드하고 실행<br>→ 인터넷 연결 없이도 AI 기능을 활용할 수 있도록 지원함

## 2. 주요 특징
- 간편한 설치 및 사용
    - 단일 바이너리 파일 형태로 배포되어 설치가 매우 간단함
    - 직관적인 명령행 인터페이스(CLI)를 제공하여 모델 다운로드, 실행, 관리가 용이함
        - 명령어 한 줄로 모델 실행 가능

            ```bash
            ollama run <모델명>
            ```

    - 필요한 모델은 자동으로 다운로드 및 설치됨
    
- 다양한 모델 지원
    - Llama 2, Llama 3, Mistral, CodeLlama 등 여러 LLM 사용 가능
    - Ollama가 관리하는 모델 레지스트리에서 쉽게 다운로드하여 사용할 수 있음
    - 사용자 필요에 따라 모델을 선택하고 커스터마이징 가능

- 로컬 실행
    - 모든 연산이 사용자의 로컬 컴퓨터(CPU 또는 GPU)에서 이루어짐<br>→ 데이터 프라이버시를 보장하고 인터넷 연결 없이도 작동
    - macOS, Windows, Linux 등 다양한 운영체제에서 실행 가능
    - Docker를 통한 배포 지원

- 레이어 기반 모델 관리
    - 모델을 레이어 형태로 관리 → 디스크 공간을 효율적으로 사용
    - 모델 업데이트 및 사용자 정의가 용이함

- Modelfile을 통한 사용자 정의
    - `Modelfile`이라는 간단한 문법 사용<br>→ 모델의 프롬프트 템플릿, 시스템 메시지, 파라미터 등을 사용자 정의할 수 있음
    - 모델의 프롬프트와 설정을 사용자 맞춤으로 조정 가능

- API 제공
    - RESTful API를 제공<br>→ 다른 애플리케이션이나 도구에서 Ollama를 통해 실행되는 모델을 쉽게 통합하여 사용할 수 있음

- GPU 가속 지원
    - NVIDIA CUDA 및 Apple Metal 지원 → GPU를 활용한 모델 추론 속도 향상 제공

## 3. 설치 방법 지원
- Windows: 현재 프리뷰 버전 제공
- Linux: 스크립트를 통해 설치 (curl 명령어 사용)
- macOS: Homebrew 또는 다운로드 페이지에서 설치
- Docker: 공식 이미지를 사용하여 컨테이너 환경에서 실행
    
## 4. 활용 분야
- 개인적인 AI 실험 및 학습
    - LLM을 직접 로컬 환경에서 실행
    - 다양한 프롬프트를 시도하며 모델의 동작 방식을 이해하는 데 유용함

- 오프라인 환경에서의 AI 활용
    - 인터넷 연결이 제한적인 환경에서도 텍스트 생성, 질의 응답, 코드 생성 등 LLM 기반 작업을 수행할 수 있음

- 데이터 프라이버시가 중요한 작업
    - 민감한 데이터를 클라우드에 전송하지 않고 로컬에서 처리하여 데이터 보안을 강화

- AI개발 및 통합
    - 개발자는 Ollama API를 통해 자신의 애플리케이션에 LLM 기능을 쉽게 통합할 수 있음

## 5. 활용 사례
- 텍스트 생성, 번역, 질의응답, 요약 등 다양한 작업
- 코드 생성 및 디버깅에 특화된 CodeLlama 같은 모델 활용 가능
- 멀티모달 입력(텍스트와 이미지) 지원