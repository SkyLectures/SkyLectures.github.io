---
layout: page
title:  "데이터 전처리 기법"
date:   2025-03-01 10:00:00 +0900
permalink: /materials/S02-02-02-01_01-DataPreprocess
categories: materials
---
* toc
{:toc .large-only .toc-sticky:true}


### 1. 단계별 데이터 전처리 기법

> - 데이터 전처리
>   - 원시 데이터를 분석과 모델링에 적합하도록 **정리하고 변환하는 핵심 단계**
>   - 데이터의 품질을 높이고, 머신러닝 모델의 성능을 향상시키며, 분석 결과의 신뢰성을 확보하는 데 필수
>   - "Garbage In, Garbage Out(쓰레기가 들어가면 쓰레기가 나온다)"
>   - 좋은 데이터를 만드는 것이 곧 좋은 결과를 만드는 첫걸음
>   - 데이터 전처리 단계는 흔히 **데이터 클리닝, 데이터 변환, 데이터 축소**의 3 단계로 구분함

#### 1.1 데이터 클리닝(Data Cleaning)

> - 데이터 클리닝: 데이터에서 불완전하거나 부정확한 부분을 식별하고 수정하여 데이터의 품질을 높이는 과정

- **결측값 처리 (Missing Value Imputation)**
    - 개념
        - 데이터에 빠져 있는 값(누락된 값)을 처리하는 기법
    - 방법
        - 제거
            - 결측값이 포함된 행이나 열을 아예 삭제
            - 데이터 손실이 클 수 있으므로 신중하게 적용
        - 대체
            - 평균, 중앙값, 최빈값으로 대체
                - 수치형 데이터는 평균이나 중앙값으로,
                - 범주형 데이터는 최빈값으로 대체하는 것이 일반적
            - 예측 모델 활용
                - 다른 변수들을 기반으로 결측값을 예측하여 채움
            - 고정된 값으로 대체
                - 0, -1 등 특정 의미를 부여하는 값으로 채움

- **이상값 처리 (Outlier Handling)**
    - 개념
        - 다른 데이터와 비교했을 때 현저하게 다른 값을 가지는 '이상치'를 식별하고 처리하는 기법
        - 이상치는 분석 결과에 왜곡을 줄 수 있음
    - 방법
        - 제거
            - 이상치를 데이터셋에서 제거
        - 변환
            - 로그 변환 등을 통해 이상치의 영향을 줄임
        - 대체/조정
            - 이상치를 다른 값(예: 사분위수 범위의 경계 값)으로 대체

- **중복 데이터 제거 (Duplicate Data Removal)**
    - 개념
        - 데이터셋 내에 동일하거나 거의 동일한 정보가 여러 번 존재하는 경우를 처리
    - 방법
        - 중복된 행을 식별하고 제거하여 데이터의 일관성 유지

- **오류 데이터 수정 (Error Correction)**
    - 개념
        - 잘못 입력되었거나 비논리적인 데이터를 수정하는 과정
            - 예: 나이가 음수, 성별이 '남'인데 임신 여부가 '예' 등
    - 방법
        - 도메인 지식 활용하거나 
        - 규칙 기반으로 오류를 검출하여 수정

#### 1.2 데이터 변환(Data Transformation)

> - 데이터 변환: 데이터를 분석 및 모델링에 더 적합한 형태로 바꾸는 과정

- **데이터 스케일링 (Data Scaling)**
    - 개념
        - 서로 다른 범위나 단위를 가진 특성(feature)들의 값 범위를 일정하게 맞춰주는 기법
        - 모델이 특정 특성에만 치우쳐 학습되는 것을 방지
    - 방법
        - 표준화 (Standardization)
            - 데이터를 평균이 0, 표준편차가 1인 정규분포 형태로 변환
            - 'StandardScaler'를 주로 사용
        - 정규화 (Normalization - Min-Max Scaling)
            - 데이터를 특정 범위(예: 0~1 또는 -1~1)로 조정
            - 'MinMaxScaler'를 주로 사용
        
        > - 데이터 정규화는 별도의 단계로 구분하는 경우도 있으나 일반적으로는 데이터 스케일링에 포함됨
        {: .expert-quote}

- **데이터 구간화 (Binning / Discretization)**
    - 개념
        - 연속형 데이터를 몇 개의 구간으로 나누어 범주형 데이터로 변환하는 기법
    - 방법
        - pd.cut
            - 지정된 구간이나 균등한 크기의 구간으로 데이터를 나눔
        - pd.qcut
            - 데이터의 분포를 비슷한 크기의 그룹으로 나눔
                - 예: 데이터 개수가 고르게 분포되도록 그룹화

- **범주형 데이터 인코딩 (Categorical Data Encoding)**
    - 개념
        - '성별', '지역'과 같이 텍스트 형태의 범주형 데이터를 모델이 이해할 수 있는 숫자 형태로 변환
    - 방법
        - 원-핫 인코딩 (One-Hot Encoding)
            - 각 범주를 독립적인 이진(0 또는 1) 특성으로 표현
                - 예: '서울', '부산' -> [1,0], [0,1]
        - 레이블 인코딩 (Label Encoding)
            - 각 범주에 순서대로 숫자 부여
                - 예: '소', '중', '대' -> 0, 1, 2

- **날짜/시간 데이터 처리 (Date/Time Feature Extraction)**
    - 개념
        - 날짜 및 시간 데이터를 모델 학습에 활용하기 좋은 형태로 분리하거나 변환
    - 방법
        - '년', '월', '일', '요일', '시간' 등으로 분리하거나,
        - 시간 간격(duration)을 계산하여
        - 새로운 특성으로 만듦

- **피처 엔지니어링 (Feature Engineering)**
    - 개념
        - 기존 특성들을 조합하거나 변형하여 새로운 의미 있는 특성(파생 변수)을 생성하는 과정
        - 모델의 성능을 극대화하기 위해 기존 데이터로부터 새로운 변수를 만들어내는 작업
    - 목적
        - 모델의 예측력을 높일 수 있는 새로운 정보 생성
        - 도메인 지식을 활용한 변수 재구성 등
    - 방법
        - $$총 매출 = 개수 * 단가$$ , $$BMI = 체중 / (키^2)$$ 와 같은 새로운 특성을 만듦

    > - 피처 엔지니어링은 중요하고 창의적인 과정이라 별도의 단계로 강조되기도 함
    {: .expert-quote}

    
#### 1.3 데이터 축소(Data Reduction)

> - 데이터 축소
>   - 데이터의 양을 줄이면서도 중요한 정보를 최대한 보존하는 기법
>   - 모델의 학습 속도를 높이고 과적합을 방지하는 데 도움

- **차원 축소 (Dimensionality Reduction)**
    - 개념
        - 데이터의 특성(변수) 개수를 줄이는 기법
    - 방법
        - 주성분 분석 (PCA, Principal Component Analysis)
            - 데이터의 분산을 가장 잘 설명하는 새로운 축을 찾아 데이터를 투영하여 차원을 줄임
        - 선형 판별 분석 (LDA, Linear Discriminant Analysis)
            - 분류 문제에서 클래스 간의 분리를 최대화하는 축을 찾아 차원을 줄임
        - 특성 선택 (Feature Selection)
            - 통계적 방법(카이제곱 검정, 상관관계 분석)이나 모델 기반 방법(Lasso, Random Forest의 특성 중요도)을 통해 가장 중요한 특성들을 직접 선택

- **데이터 샘플링 (Data Sampling)**
    - 개념
        - 전체 데이터셋 중 일부를 선택하여 모델 학습에 사용하는 기법
        - 특히 불균형한 클래스를 가진 데이터셋에서 중요함
            - 예: 사기 거래 탐지, 질병 진단
    - 방법
        - 언더샘플링 (Undersampling): 다수 클래스의 데이터 수를 줄임
        - 오버샘플링 (Oversampling): 소수 클래스의 데이터 수를 늘림 (예: SMOTE)

## 2. 기타 전처리 단계

> - 3가지 단계 외에, 전처리 과정에서 중요하게 다뤄지거나 때로는 별도의 단계로 간주되는 기법

- **데이터 통합 (Data Integration)**
    - 개념
        - 여러 이질적인 데이터 소스(데이터베이스, 파일, 웹 등)에서 데이터를 가져와 일관된 하나의 저장소로 결합하는 과정
        - 전처리 단계에 들어가기 전, 또는 클리닝 단계의 일부로 간주될 수 있음
    - 목적
        - 데이터 중복 제거, 데이터 충돌 해결, 일관된 데이터 구조 구축 등을 통해 데이터를 분석 가능한 형태로 만듦

- **데이터 증강 (Data Augmentation)**
    - 개념
        - 특히 이미지, 오디오, 텍스트 데이터와 같은 분야에서 데이터셋의 양을 늘리기 위해
        - 기존 데이터를 약간 변형하여 새로운 데이터를 생성하는 기법
    - 목적
        - 모델의 과적합 방지
        - 일반화 성능 향상

- **데이터 품질 평가 (Data Quality Assessment)**
    - 개념
        - 전처리 *과정* 자체라기보다는,
        - 전처리의 효율성을 결정하고
        - 전처리 전후 데이터의 신뢰성을 확인하는 중요한 *검증 단계*
        - 데이터가 분석에 적합한 품질을 가지고 있는지 지속적으로 평가함
    - 목적
        - 데이터 클리닝의 필요성 판단
        - 전처리 효과 검증
        - 최종 데이터셋의 신뢰도 확보

> - 데이터 전처리 단계는 데이터 클리닝 **→** 데이터 변환 **→** 데이터 축소와 같이 일반적으로 흐르지만, 실제 프로젝트에서는 이러한 단계들이 엄격하게 순차적이기보다는 상호 연관되어 반복적으로 수행될 수 있음
> - 예를 들어, 데이터 통합 후 클리닝을 하고, 다시 피처 엔지니어링을 위해 데이터를 변환하는 과정에서 필요하다면 다시 클리닝으로 돌아갈 수도 있음
> - 중요한 것은 각 기법이 데이터를 어떤 목적으로, 어떻게 개선하는지 이해하는 것임
{: .expert-quote}