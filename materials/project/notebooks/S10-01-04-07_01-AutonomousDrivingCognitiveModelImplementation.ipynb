{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **UPerNet(ResNet-50) 기반의 Semantic Segmentation**"
      ],
      "metadata": {
        "id": "CQ4exVWLq-wY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1. 실습용 리소스 가져오기**"
      ],
      "metadata": {
        "id": "xWkJ-Kd-zSx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "import os"
      ],
      "metadata": {
        "id": "-gcu8KaUOpvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir bdd10k\n",
        "!mkdir pretrained"
      ],
      "metadata": {
        "id": "S1ncWe8OAo3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_id = '1Dap2l5I9R17fXmZI3HNUFyYos9FgTdvG'\n",
        "gdown.download(id=dataset_id, output='/content/bdd10k/bdd10k.zip')\n",
        "os.system(\"unzip /content/bdd10k/bdd10k.zip\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dh6w1OC5LMTM",
        "outputId": "6fce2e80-4b9a-4e6d-e48e-7087aa8d432e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1Dap2l5I9R17fXmZI3HNUFyYos9FgTdvG\n",
            "From (redirected): https://drive.google.com/uc?id=1Dap2l5I9R17fXmZI3HNUFyYos9FgTdvG&confirm=t&uuid=f0b0992e-3034-4a77-8ce2-ad7624fb5137\n",
            "To: /content/bdd10k/bdd10k.zip\n",
            "100%|██████████| 1.19G/1.19G [00:17<00:00, 68.4MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm ./bdd10k/bdd10k.zip"
      ],
      "metadata": {
        "id": "mm48BsANBF08"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_model_id = '1h1T_JZtauItTeis7MHNCm600idZ2TJYi'\n",
        "gdown.download(id=pretrained_model_id, output='/content/pretrained/upernet_r50-d8_769x769_40k_sem_seg_bdd100k.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "id": "spghryM_qrMk",
        "outputId": "f8c90535-1f51-4827-d791-01798341a03c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1h1T_JZtauItTeis7MHNCm600idZ2TJYi\n",
            "From (redirected): https://drive.google.com/uc?id=1h1T_JZtauItTeis7MHNCm600idZ2TJYi&confirm=t&uuid=199dd7f4-1351-4273-af21-bad9283370e3\n",
            "To: /content/pretrained/upernet_r50-d8_769x769_40k_sem_seg_bdd100k.pth\n",
            "100%|██████████| 266M/266M [00:03<00:00, 82.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./pretrained/upernet_r50-d8_769x769_40k_sem_seg_bdd100k.pth'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2. PyTorch 기반 구현 실습**"
      ],
      "metadata": {
        "id": "6u3Kp7pMrgo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.1 패키지 가져오기"
      ],
      "metadata": {
        "id": "BY2qHc5MrxOS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import json\n",
        "import time"
      ],
      "metadata": {
        "id": "D8SdlzHtoO-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2"
      ],
      "metadata": {
        "id": "AS3WV0-UocaJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50, ResNet50_Weights"
      ],
      "metadata": {
        "id": "NCd3MorBoSZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 기반 환경 설정"
      ],
      "metadata": {
        "id": "YH_D5JUmr06y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터셋 및 사전학습모델 경로 설정"
      ],
      "metadata": {
        "id": "EmulF3-Wr6UD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BDD10K_DATA_ROOT_PATH = \"/content/bdd10k\""
      ],
      "metadata": {
        "id": "JuTabs-doST9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PRETRAINED_MODEL_PATH = \"/content/pretrained/upernet_r50-d8_769x769_40k_sem_seg_bdd100k.pth\""
      ],
      "metadata": {
        "id": "niSBR8bOoSN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BDD10K Segmentation 클래스 정의\n",
        "    - BDD10K 데이터셋의 원본 Semantic Segmentation 라벨 구조\n",
        "        - 주행 가능 영역과 차선에 대해 상당히 세분화된 픽셀 라벨을 제공\n",
        "            - drivable_maps (주행 가능 영역 마스크):\n",
        "                - 픽셀 값 0: background (배경, 즉 주행 불가능한 영역)\n",
        "                - 픽셀 값 1: direct (현재 차량이 직접 주행할 수 있는 영역)\n",
        "                - 픽셀 값 2: alternative (차량은 주행할 수 있지만, 현재 차선이 아닌 다른 주행 가능 영역)\n",
        "                - 목적: 차량이 안전하게 이동할 수 있는 공간 파악\n",
        "            - lane_masks (차선 마스크):\n",
        "                - 픽셀 값 0: background (배경)\n",
        "                - 픽셀 값 1: road_line (도로의 기본적인 차선 경계선)\n",
        "                - 픽셀 값 2: other_line (횡단보도 선, 정지선 등 기타 도로 위의 선)\n",
        "                - 목적: 차량의 횡방향 위치 제어(차선 유지)와 차선 변경 판단에 사용\n",
        "    - 실습에 적용된 구조\n",
        "        -  실습 목적: **차선 및 주행 가능 영역 인식을 이해**하는 것\n",
        "        - background: 0: 주행과 무관한 모든 픽셀 (가장 낮은 ID)\n",
        "        - drivable: 1: 차량이 안전하게 주행할 수 있는 모든 영역 (원본의 direct와 alternative를 모두 포함)\n",
        "        - lane: 2: 도로 위의 모든 차선 (원본의 road_line과 other_line을 모두 포함)"
      ],
      "metadata": {
        "id": "jxxGs33n03iN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLASS_LABELS = {\n",
        "    \"background\": 0,\n",
        "    \"drivable\": 1,\n",
        "    \"lane\": 2,\n",
        "    # \"road_line\": 2,\n",
        "    # \"other_line\": 3\n",
        "}"
      ],
      "metadata": {
        "id": "MrCUEIbBoSHs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 분류 클래스의 개수 = 배경 + 주행 가능 영역 + 차선 = 3"
      ],
      "metadata": {
        "id": "iMwl9Tg7t3M9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CLASSES = len(CLASS_LABELS)"
      ],
      "metadata": {
        "id": "klLyHC0poR7d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- UPerNet 모델의 입력 이미지 크기 (사전 학습 모델과 동일하게 맞춤)"
      ],
      "metadata": {
        "id": "bozHMB17ty4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH = 769, 769"
      ],
      "metadata": {
        "id": "hLgL39bGo17b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- GPU 설정"
      ],
      "metadata": {
        "id": "VbVRC5QLtwhY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GopjXcZ_o3kh",
        "outputId": "4d846110-7ee6-4ea7-ef58-2133ab0b97ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 구성요소 구현"
      ],
      "metadata": {
        "id": "lMJGc8nCw3cC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.1 BDD10K Semantic Segmentation 데이터셋 로더"
      ],
      "metadata": {
        "id": "cE9NJDlguA3q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BDD10K 데이터셋의 Semantic Segmentation 라벨 구조에 맞춰 수정\n",
        "- JSON에 \"segmentation\" 필드가 직접 마스크 경로를 제공하지 않으므로, 규칙 기반으로 'drivable_maps'와 'lane_masks' 폴더에서 마스크를 찾아야 함\n",
        "- 예시\n",
        "    - 원본 이미지: 'val/b1c4c1a2-3f2d0111.jpg'\n",
        "    - drivable 마스크: 'labels/drivable_maps/10k/val/b1c4c1a2-3f2d0111.png'\n",
        "    - lane 마스크: 'labels/lane_masks/10k/val/b1c4c1a2-3f2d0111.png'"
      ],
      "metadata": {
        "id": "RFp-clvKuFTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_bdd10k_segmentation_paths(split='val'):\n",
        "    if split == 'train':\n",
        "        image_dir = os.path.join(BDD10K_DATA_ROOT_PATH, 'train')\n",
        "        drivable_mask_dir = os.path.join(BDD10K_DATA_ROOT_PATH, 'labels', 'drivable_maps', '10k', 'train')\n",
        "        lane_mask_dir = os.path.join(BDD10K_DATA_ROOT_PATH, 'labels', 'lane_masks', '10k', 'train')\n",
        "\n",
        "    elif split == 'val':\n",
        "        image_dir = os.path.join(BDD10K_DATA_ROOT_PATH, 'val')\n",
        "        drivable_mask_dir = os.path.join(BDD10K_DATA_ROOT_PATH, 'labels', 'drivable_maps', '10k', 'val')\n",
        "        lane_mask_dir = os.path.join(BDD10K_DATA_ROOT_PATH, 'labels', 'lane_masks', '10k', 'val')\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"지원하지 않는 split: {split}. 'train', 'val' 중 하나여야 합니다.\")\n",
        "\n",
        "    samples = []\n",
        "\n",
        "    # 이미지 파일들을 기준으로 마스크를 찾음\n",
        "    for image_name in os.listdir(image_dir):\n",
        "        if image_name.endswith('.jpg'):\n",
        "            base_name = os.path.splitext(image_name)[0]\n",
        "\n",
        "            image_path = os.path.join(image_dir, image_name)\n",
        "            drivable_mask_path = os.path.join(drivable_mask_dir, base_name + '.png')\n",
        "            lane_mask_path = os.path.join(lane_mask_dir, base_name + '.png')\n",
        "\n",
        "            # 모든 파일이 존재하는지 확인 (필수)\n",
        "            if os.path.exists(image_path) and os.path.exists(drivable_mask_path) and os.path.exists(lane_mask_path):\n",
        "                samples.append({\n",
        "                    'image_path': image_path,\n",
        "                    'drivable_mask_path': drivable_mask_path,\n",
        "                    'lane_mask_path': lane_mask_path,\n",
        "                    'image_name': image_name\n",
        "                })\n",
        "\n",
        "            # else:\n",
        "            #     print(f\"누락된 파일: {image_name} 관련 마스크 파일이 없습니다.\")\n",
        "\n",
        "    # 안정적인 학습/추론을 위해 리스트를 정렬\n",
        "    samples = sorted(samples, key=lambda x: x['image_name'])\n",
        "    return samples"
      ],
      "metadata": {
        "id": "8e6FJaoVo45l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 학습, 검증을 하지 않고 사전학습된 모델을 로드하여 이용할 때에는 단순히 파일만 가져오도록 함\n",
        "    - 'val' 디렉토리의 JPG 파일 목록만 가져옴"
      ],
      "metadata": {
        "id": "ctq7sP_U3weP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_image_paths_from_dir(target_dir):\n",
        "    \"\"\"\n",
        "    지정된 디렉토리에서 모든 JPG 이미지 파일의 경로를 가져옵니다.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(target_dir):\n",
        "        print(f\"CRITICAL ERROR: Directory does not exist: {target_dir}\")\n",
        "        return []\n",
        "\n",
        "    image_paths = []\n",
        "    for fname in os.listdir(target_dir):\n",
        "        if fname.lower().endswith(('.jpg', '.jpeg')):\n",
        "            image_paths.append(os.path.join(target_dir, fname))\n",
        "\n",
        "    if not image_paths:\n",
        "        print(f\"WARNING: No JPG/JPEG images found in {target_dir}. Check directory content or file extensions.\")\n",
        "        return []\n",
        "\n",
        "    # 랜덤 선택을 위해 이미지 이름을 기준으로 정렬은 불필요하지만 일관성을 위해 유지\n",
        "    return sorted(image_paths)"
      ],
      "metadata": {
        "id": "QLeDAjHt35pf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BDD10KSegmentationDataset 클래스\n",
        "    - 마스크를 합쳐 최종 Segmentation Target 생성\n",
        "        - BDD10K의 기준\n",
        "            - drivable_maps\n",
        "                - 0: background, 1: direct drivable, 2: alternative drivable\n",
        "            - lane_masks\n",
        "                - 0: background, 1: road line, 2: other lane line\n",
        "        - 실습에서의 기준(단순화)\n",
        "            - 0: background, 1: drivable area, 2: lane_lines"
      ],
      "metadata": {
        "id": "fI5ce7tevCBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# BDD10KSegmentationDataset 클래스는 학습시에만 사용되므로 주석 처리하거나 제거 가능\n",
        "class BDD10KSegmentationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, split='val', transform=None, target_transform=None):\n",
        "        self.samples = get_bdd10k_segmentation_paths(split)\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform # 마스크 변환용\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample_info = self.samples[idx]\n",
        "\n",
        "        # 이미지 로드 (RGB)\n",
        "        image = Image.open(sample_info['image_path']).convert('RGB')\n",
        "\n",
        "        # 주행 가능 영역 마스크 로드 (Grayscale)\n",
        "        drivable_mask = Image.open(sample_info['drivable_mask_path']).convert('L')\n",
        "        # 차선 마스크 로드 (Grayscale)\n",
        "        lane_mask = Image.open(sample_info['lane_mask_path']).convert('L')\n",
        "\n",
        "        # 이미지 크기와 동일한 최종 마스크 생성 (all zeros initially for background)\n",
        "        final_mask_np = np.zeros(image.size[::-1], dtype=np.uint8) # (H, W)\n",
        "\n",
        "        drivable_mask_np = np.array(drivable_mask)\n",
        "        lane_mask_np = np.array(lane_mask)\n",
        "\n",
        "        # 1. 주행 가능 영역 (ID 1) 설정\n",
        "        # drivable_mask_np의 픽셀 값이 1 (direct) 또는 2 (alternative)인 부분을 ID 1로 설정\n",
        "        final_mask_np[np.where(drivable_mask_np > 0)] = CLASS_LABELS[\"drivable\"] # drivable_mask_np > 0\n",
        "\n",
        "        # 2. 차선 (ID 2) 설정 - 차선은 주행 가능 영역 위에 덮어씌움 (더 중요한 요소)\n",
        "        # lane_mask_np의 픽셀 값이 1 (road line) 또는 2 (other lane line)인 부분을 ID 2로 설정\n",
        "        final_mask_np[np.where(lane_mask_np > 0)] = CLASS_LABELS[\"lane\"] # lane_mask_np > 0\n",
        "\n",
        "        target_mask = Image.fromarray(final_mask_np)\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            target_mask = self.target_transform(target_mask)\n",
        "        else: # target_transform이 없으면 기본적으로 텐서로 변환\n",
        "            target_mask = torch.from_numpy(np.array(target_mask, dtype=np.int64))\n",
        "\n",
        "        return image, target_mask, sample_info['image_name'] # image_name도 반환하여 추론 결과에 사용"
      ],
      "metadata": {
        "id": "6rPiCMXfo41u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.2 Minimal UPerNet (ResNet50 백본) 구현"
      ],
      "metadata": {
        "id": "j3cp2bo6v-o-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- torchvision의 ResNet50을 사용하고, FPN과 PPM을 간략화하여 구현\n",
        "    - FPN (Feature Pyramid Network)\n",
        "    - PPM (Pyramid Pooling Module)\n"
      ],
      "metadata": {
        "id": "dTy1_jhVwFT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPM(nn.Module):\n",
        "    def __init__(self, in_dim, reduction_dim, bins):\n",
        "        super(PPM, self).__init__()\n",
        "        self.features = []\n",
        "        for bin in bins:\n",
        "            self.features.append(nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(bin),\n",
        "                nn.Conv2d(in_dim, reduction_dim, kernel_size=1, bias=False),\n",
        "                nn.BatchNorm2d(reduction_dim),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ))\n",
        "        self.features = nn.ModuleList(self.features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_size = x.size()\n",
        "        out = [x]\n",
        "        for f in self.features:\n",
        "            out.append(F.interpolate(f(x), x_size[2:], mode='bilinear', align_corners=True))\n",
        "        return torch.cat(out, 1)"
      ],
      "metadata": {
        "id": "EYtN1ATOo4xo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class UPerNet(nn.Module):\n",
        "    def __init__(self, num_classes, backbone_name='resnet50', pretrained_backbone=True):\n",
        "        super(UPerNet, self).__init__()\n",
        "\n",
        "        # 백본 (Feature Extractor) - ResNet50 사용\n",
        "        if backbone_name == 'resnet50':\n",
        "            # weights=ResNet50_Weights.IMAGENET1K_V1: ImageNet으로 사전 학습된 가중치 로드\n",
        "            resnet = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1 if pretrained_backbone else None)\n",
        "\n",
        "            # ResNet의 각 Stage에서 특징맵을 추출\n",
        "            # C1 = conv1, bn1, relu, maxpool\n",
        "            # C2 = layer1\n",
        "            # C3 = layer2\n",
        "            # C4 = layer3\n",
        "            # C5 = layer4\n",
        "\n",
        "            # 실제 FPN은 C2, C3, C4, C5를 사용함\n",
        "            # `self.resnet_features`는 FPN에 직접 전달될 특징 맵들을 저장함\n",
        "            self.backbone = nn.Sequential(\n",
        "                resnet.conv1, resnet.bn1, resnet.relu, resnet.maxpool, # Initial layers (before C2)\n",
        "                resnet.layer1, # C2 output: 256\n",
        "                resnet.layer2, # C3 output: 512\n",
        "                resnet.layer3, # C4 output: 1024\n",
        "                resnet.layer4  # C5 output: 2048\n",
        "            )\n",
        "\n",
        "            # ResNet Stage별 출력 채널\n",
        "            # ResNet50: C2=256, C3=512, C4=1024, C5=2048\n",
        "            self.in_channels = [256, 512, 1024, 2048]\n",
        "        else:\n",
        "            raise NotImplementedError(f\"Backbone {backbone_name} not supported yet.\")\n",
        "\n",
        "        # --- FPN & PPM 관련 채널 설정 재확인 ---\n",
        "        # UPerNet은 PPM의 출력을 가장 높은 피라미드 레벨의 FPN에 통합합니다.\n",
        "        # 즉, C5 특징맵을 PPM에 넣고, 그 결과를 FPN의 시작점으로 사용합니다.\n",
        "\n",
        "        self.ppm_out_channels = 512 # PPM의 최종 출력 채널\n",
        "        self.ppm = PPM(self.in_channels[-1], self.ppm_out_channels // 4, bins=(1, 2, 3, 6))\n",
        "        self.ppm_conv = nn.Sequential(\n",
        "            nn.Conv2d(self.in_channels[-1] + self.ppm_out_channels, self.ppm_out_channels, kernel_size=3, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(self.ppm_out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # FPN 입력 채널들: C2, C3, C4\n",
        "        # self.in_channels[:-1]은 [256, 512, 1024]\n",
        "        self.fpn_in_channels = self.in_channels[:-1] # ResNet C2, C3, C4\n",
        "        self.fpn_out_channels = 256 # FPN 각 단계의 출력 채널 (일반적으로 256)\n",
        "\n",
        "        self.fpn_convs = nn.ModuleList() # 1x1 conv for FPN lateral connections\n",
        "        self.fpn_post_convs = nn.ModuleList() # 3x3 conv for FPN output features\n",
        "\n",
        "        for in_dim in reversed(self.fpn_in_channels): # C4(1024) -> C3(512) -> C2(256) 순서로 처리\n",
        "            self.fpn_convs.append(nn.Conv2d(in_dim, self.fpn_out_channels, kernel_size=1, bias=False))\n",
        "            self.fpn_post_convs.append(nn.Sequential(\n",
        "                nn.Conv2d(self.fpn_out_channels, self.fpn_out_channels, kernel_size=3, padding=1, bias=False),\n",
        "                nn.BatchNorm2d(self.fpn_out_channels),\n",
        "                nn.ReLU(inplace=True)\n",
        "            ))\n",
        "\n",
        "        # 최종 분류기 (Classifier)\n",
        "        # PPM 출력 채널 + 모든 FPN 출력 채널을 합친 후 num_classes 채널로\n",
        "        self.final_head = nn.Sequential(\n",
        "            nn.Conv2d(self.ppm_out_channels + len(self.fpn_in_channels) * self.fpn_out_channels,\n",
        "                      num_classes, kernel_size=1)\n",
        "        )\n",
        "\n",
        "    def _forward_backbone(self, x):\n",
        "        # ResNet의 각 Stage에서 특징맵을 추출\n",
        "        # C1 = conv1, bn1, relu, maxpool\n",
        "        # C2 = layer1\n",
        "        # C3 = layer2\n",
        "        # C4 = layer3\n",
        "        # C5 = layer4\n",
        "\n",
        "        # torchvision resnet의 경우, layer1, layer2, layer3, layer4가 각각 C2, C3, C4, C5에 해당\n",
        "        x = self.backbone[0](x) # conv1, bn1, relu, maxpool\n",
        "        c2 = self.backbone[1](x) # layer1\n",
        "        c3 = self.backbone[2](c2) # layer2\n",
        "        c4 = self.backbone[3](c3) # layer3\n",
        "        c5 = self.backbone[4](c4) # layer4\n",
        "\n",
        "        return [c2, c3, c4, c5] # List of feature maps from C2 to C5\n",
        "\n",
        "    def forward(self, x):\n",
        "        input_size = x.size()[2:] # (H, W)\n",
        "\n",
        "        # 백본을 통해 특징맵 추출\n",
        "        c_features = self._forward_backbone(x) # [c2, c3, c4, c5]\n",
        "\n",
        "        # C5 특징맵에 PPM 적용\n",
        "        ppm_out = self.ppm(c_features[-1]) # c_features[-1]은 c5\n",
        "        ppm_out = self.ppm_conv(ppm_out) # (B, ppm_out_channels, H_c5, W_c5)\n",
        "\n",
        "        # FPN (Feature Pyramid Network)\n",
        "        # top-down path\n",
        "        fpn_out_list = [ppm_out] # PPM 출력이 FPN의 가장 높은 레벨 출력으로 시작\n",
        "\n",
        "        # ResNet 특징 맵은 [C2, C3, C4, C5] 순서\n",
        "        # FPN은 C4 -> C3 -> C2 역순으로 합쳐나감.\n",
        "        # c_features[:-1]은 [c2, c3, c4]\n",
        "        # reversed(self.fpn_in_channels)는 [1024, 512, 256]\n",
        "\n",
        "        current_fpn_feature = ppm_out # P5 (C5)에서 시작하는 FPN 특징\n",
        "\n",
        "        # Zip fpn_convs with reversed(c_features[:-1])\n",
        "        # self.fpn_convs는 (C4->256), (C3->256), (C2->256)\n",
        "        # c_features[:-1]은 [C2, C3, C4]\n",
        "\n",
        "        # 루프를 돌면서 C4, C3, C2에 해당하는 특징맵을 사용해야 합니다.\n",
        "        # c_features[-2]는 C4, c_features[-3]은 C3, c_features[-4]는 C2\n",
        "\n",
        "        for i, lateral_conv in enumerate(self.fpn_convs):\n",
        "            # 이전 FPN 레벨의 특징맵을 현재 스케일로 upsample\n",
        "            # 현재 current_fpn_feature의 스케일 (H, W)\n",
        "            target_size = c_features[-(i+2)].size()[2:] # 예를 들어, i=0일 때 c_features[-2]는 C4\n",
        "            upsampled_current_fpn_feature = F.interpolate(current_fpn_feature, size=target_size, mode='bilinear', align_corners=True)\n",
        "\n",
        "            # 현재 ResNet 특징맵 (C4, C3, C2)을 1x1 컨볼루션으로 채널 맞춤 (lateral connection)\n",
        "            # 여기였던 self.fpn_in_convs[i](c[i+1])가 문제였는데, c_features[-(i+2)]로 직접 참조합니다.\n",
        "            # self.fpn_convs[i]는 lateral_conv에 해당\n",
        "            lateral_feature = lateral_conv(c_features[-(i+2)]) # c_features[-2]는 C4, c_features[-3]는 C3, c_features[-4]는 C2\n",
        "\n",
        "            # FPN Add\n",
        "            current_fpn_feature = lateral_feature + upsampled_current_fpn_feature\n",
        "\n",
        "            # 3x3 conv (post-fusion)\n",
        "            current_fpn_feature = self.fpn_post_convs[i](current_fpn_feature)\n",
        "\n",
        "            fpn_out_list.append(current_fpn_feature)\n",
        "\n",
        "        # UPerNet은 모든 FPN 레벨의 출력을 원래 입력 크기로 upsample한 후 concatenate\n",
        "        # fpn_out_list에는 [PPM_out(P5), P4, P3, P2] 순서로 담겨있습니다.\n",
        "\n",
        "        all_upsampled_fpn_features = []\n",
        "        for feature in fpn_out_list:\n",
        "            all_upsampled_fpn_features.append(F.interpolate(feature, size=input_size, mode='bilinear', align_corners=True))\n",
        "\n",
        "        # 모든 업샘플링된 특징들을 채널 방향으로 합칩니다.\n",
        "        concat_features = torch.cat(all_upsampled_fpn_features, dim=1) # (B, total_channels, H, W)\n",
        "\n",
        "        # 최종 분류기 헤드\n",
        "        output = self.final_head(concat_features)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "afEmPu01o4k_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.3 데이터 전처리 및 후처리 변환"
      ],
      "metadata": {
        "id": "l7YPwqeNwO2o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageNet으로 사전 학습된 ResNet50 백본을 위한 정규화 값 사용\n",
        "TRANSFORM_IMG = transforms.Compose([\n",
        "    transforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH)),\n",
        "    transforms.ToTensor(), # (H, W, C) -> (C, H, W), 0-255 -> 0-1\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# 마스크는 Tensor로 변환하고, 픽셀 값 그대로 사용 (클래스 ID)\n",
        "TRANSFORM_MASK = transforms.Compose([\n",
        "    transforms.Resize((INPUT_IMAGE_HEIGHT, INPUT_IMAGE_WIDTH), interpolation=Image.NEAREST), # 마스크는 Nearest Neighbor 보간\n",
        "    transforms.ToTensor(), # 0-255 -> 0-1.0\n",
        "    # 마스크는 클래스 ID이므로 정규화하지 않음. ToTensor() 이후 (1, H, W) 형태로 float32\n",
        "    # 나중에 .long()로 int64로 변환하여 Loss 함수에 전달해야 함.\n",
        "])"
      ],
      "metadata": {
        "id": "D50aKbmypGle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.4 결과 시각화 유틸리티 함수"
      ],
      "metadata": {
        "id": "YH6eROmMwTz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "COLOR_MAP = {\n",
        "    0: (0, 0, 0),       # Background (Black)\n",
        "    1: (0, 255, 0),     # Drivable Area (Green)\n",
        "    2: (255, 0, 0),     # Lane Lines (Red)\n",
        "    # 3: (0, 0, 255)    # Other lines (Blue) (사용 안 함)\n",
        "}"
      ],
      "metadata": {
        "id": "kePGx7rppGfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decode_segmentation_map(mask):\n",
        "    \"\"\"\n",
        "    예측된 클래스 ID 마스크 (numpy array)를 컬러 이미지로 변환\n",
        "    \"\"\"\n",
        "    height, width = mask.shape\n",
        "    color_mask = np.zeros((height, width, 3), dtype=np.uint8)\n",
        "    for class_id, color in COLOR_MAP.items():\n",
        "        color_mask[mask == class_id] = color\n",
        "    return color_mask"
      ],
      "metadata": {
        "id": "8femNao6pGW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 학습 부분 (코드로만 제시, 실행하지 않음)"
      ],
      "metadata": {
        "id": "J117iW6CxJVx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_upernet_model(model, train_loader, val_loader, num_epochs=10, learning_rate=0.001):\n",
        "    model.train() # 모델을 훈련 모드로 전환\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=CLASS_LABELS[\"background\"]) # 배경 픽셀은 Loss 계산에서 제외\n",
        "\n",
        "    print(\"\\n--- UPerNet 모델 학습 시작 (실제로 실행되지 않음) ---\")\n",
        "    print(\"BDD10K 데이터셋으로 UPerNet을 학습시키려면 상당한 시간과 GPU 자원이 필요합니다.\")\n",
        "    print(\"이 코드는 개념 이해를 위한 것이며, 실제 학습은 다음처럼 진행될 것입니다:\\n\")\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for i, (images, masks, _) in enumerate(train_loader):\n",
        "            images = images.to(device)\n",
        "            masks = masks.squeeze(1).long().to(device) # (B, 1, H, W) -> (B, H, W) for CrossEntropyLoss\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, masks)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            if (i + 1) % 100 == 0:\n",
        "                print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {running_loss / (i+1):.4f}\")\n",
        "\n",
        "        # 검증 루프 (생략)\n",
        "        # model.eval()\n",
        "        # with torch.no_grad(): ...\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] 평균 Loss: {running_loss / len(train_loader):.4f}\")\n",
        "        # 모델 저장 (예: torch.save(model.state_dict(), f\"upernet_epoch_{epoch+1}.pth\"))\n",
        "\n",
        "    print(\"\\n--- UPerNet 모델 학습 완료 (가정) ---\")\n"
      ],
      "metadata": {
        "id": "AmD2yiMspPT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5 사전 학습 모델 로드 및 추론 (핵심 실행 부분)"
      ],
      "metadata": {
        "id": "-XYmBbdCxRQV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BDD100K 데이터셋을 대상으로 Validation까지 수행할 때 사용"
      ],
      "metadata": {
        "id": "K8zHYeG-2RSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_segmentation_inference(model, val_dataset, num_samples_to_show=5):\n",
        "    model.eval() # 모델을 평가 모드로 전환\n",
        "\n",
        "    print(f\"\\n--- 사전 학습 모델 로드 및 추론 시작 ---\")\n",
        "    if not os.path.exists(PRETRAINED_MODEL_PATH):\n",
        "        print(f\"오류: 사전 학습 모델 파일이 없습니다: {PRETRAINED_MODEL_PATH}\")\n",
        "        print(\"파일 경로를 확인하거나, 해당 파일을 다운로드하여 스크립트와 같은 위치에 배치하세요.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=device), strict=False) # strict=False는 일부 레이어가 없을 때 유연하게 처리\n",
        "        print(f\"'{PRETRAINED_MODEL_PATH}' 모델 가중치를 성공적으로 로드했습니다.\")\n",
        "    except Exception as e:\n",
        "        print(f\"모델 가중치 로드 중 오류 발생: {e}\")\n",
        "        print(\"모델 아키텍처와 .pth 파일의 가중치가 호환되는지 확인하세요.\")\n",
        "        return\n",
        "\n",
        "    plt.figure(figsize=(20, num_samples_to_show * 5))\n",
        "\n",
        "    # DataLoader를 사용하여 배치 단위로 이미지 가져오기\n",
        "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
        "\n",
        "    sample_count = 0\n",
        "    with torch.no_grad(): # 추론 시에는 그라디언트 계산 비활성화\n",
        "        for i, (images, masks_gt, image_name) in enumerate(val_loader):\n",
        "            if sample_count >= num_samples_to_show:\n",
        "                break\n",
        "\n",
        "            images = images.to(device)\n",
        "            # masks_gt = masks_gt.squeeze(1).long().to(device) # Ground Truth 마스크 (옵션)\n",
        "\n",
        "            # 추론 실행\n",
        "            outputs = model(images) # (B, num_classes, H, W)\n",
        "\n",
        "            # 예측된 마스크 (가장 높은 확률을 가진 클래스 ID)\n",
        "            predicted_mask = torch.argmax(outputs, dim=1).squeeze(0).cpu().numpy() # (H, W)\n",
        "\n",
        "            # 원본 이미지 (PyTorch Tensor -> NumPy RGB)\n",
        "            original_image_np = images.squeeze(0).cpu().numpy()\n",
        "            original_image_np = np.transpose(original_image_np, (1, 2, 0)) # (C, H, W) -> (H, W, C)\n",
        "            original_image_np = (original_image_np * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]) * 255\n",
        "            original_image_np = original_image_np.astype(np.uint8)\n",
        "\n",
        "            # 예측된 마스크를 컬러로 디코딩\n",
        "            predicted_color_mask = decode_segmentation_map(predicted_mask)\n",
        "\n",
        "            # 원본 이미지와 예측된 마스크를 합성하여 시각화\n",
        "            # 알파 블렌딩 (원본 이미지는 RGB, 마스크는 컬러, 두 개를 투명하게 합성)\n",
        "            blended_image = cv2.addWeighted(original_image_np, 0.7, predicted_color_mask, 0.3, 0)\n",
        "\n",
        "            plt.subplot(num_samples_to_show, 1, sample_count + 1)\n",
        "            plt.imshow(blended_image)\n",
        "            plt.title(f\"Segmentation Result for {image_name[0]}\")\n",
        "            plt.axis('off')\n",
        "\n",
        "            sample_count += 1\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"--- 추론 및 시각화 완료 ---\")"
      ],
      "metadata": {
        "id": "iKzd579HpPLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- BDD10K 등을 대상으로 실제 예측에만 사용하기위하여 샘플링한 데이터에만 추론 예측을 적용함"
      ],
      "metadata": {
        "id": "FTg0Qyyl2Xv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_segmentation_inference_on_random_samples(model, image_paths_list, preprocess_transform, num_samples_to_show=5):\n",
        "    model.eval() # 모델을 평가 모드로 전환\n",
        "\n",
        "    print(f\"\\n--- 사전 학습 모델 로드 및 랜덤 {num_samples_to_show}개 샘플 추론 시작 ---\")\n",
        "    if not os.path.exists(PRETRAINED_MODEL_PATH):\n",
        "        print(f\"오류: 사전 학습 모델 파일이 없습니다: {PRETRAINED_MODEL_PATH}\")\n",
        "        print(\"파일 경로를 확인하거나, 해당 파일을 다운로드하여 스크립트와 같은 위치에 배치하세요.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        model.load_state_dict(torch.load(PRETRAINED_MODEL_PATH, map_location=device), strict=False)\n",
        "        print(f\"'{PRETRAINED_MODEL_PATH}' 모델 가중치를 성공적으로 로드했습니다.\")\n",
        "    except Exception as e:\n",
        "        print(f\"모델 가중치 로드 중 오류 발생: {e}\")\n",
        "        print(\"모델 아키텍처와 .pth 파일의 가중치가 호환되는지 확인하세요.\")\n",
        "        return\n",
        "\n",
        "    if len(image_paths_list) < num_samples_to_show:\n",
        "        print(f\"경고: 사용 가능한 이미지({len(image_paths_list)}개)가 요청한 수({num_samples_to_show}개)보다 적습니다. 사용 가능한 모든 이미지를 추론합니다.\")\n",
        "        samples_to_infer_paths = image_paths_list\n",
        "    else:\n",
        "        samples_to_infer_paths = random.sample(image_paths_list, num_samples_to_show) # 랜덤으로 샘플 선택\n",
        "\n",
        "    plt.figure(figsize=(20, num_samples_to_show * 5))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, image_path in enumerate(samples_to_infer_paths):\n",
        "            image_name = os.path.basename(image_path)\n",
        "\n",
        "            # 이미지 로드 (RGB)\n",
        "            original_image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "            # 전처리\n",
        "            input_tensor = preprocess_transform(original_image).unsqueeze(0).to(device) # (1, C, H, W)\n",
        "\n",
        "            # 추론 실행\n",
        "            start_time = time.time()\n",
        "            outputs = model(input_tensor) # (B, num_classes, H, W)\n",
        "            inference_time = time.time() - start_time\n",
        "\n",
        "            # 예측된 마스크 (가장 높은 확률을 가진 클래스 ID)\n",
        "            predicted_mask = torch.argmax(outputs, dim=1).squeeze(0).cpu().numpy() # (H, W)\n",
        "\n",
        "            # 원본 이미지 (PIL Image -> NumPy RGB, 시각화를 위해 원본 크기 유지)\n",
        "            original_image_np = np.array(original_image)\n",
        "\n",
        "            # 예측된 마스크를 원본 이미지 크기로 리사이즈 후 컬러 디코딩\n",
        "            # 모델 출력(predicted_mask)은 769x769, 원본 이미지는 1280x720\n",
        "            # 마스크를 원본 크기로 리사이즈해야 정확히 오버레이 가능\n",
        "            predicted_mask_resized = cv2.resize(predicted_mask.astype(np.uint8),\n",
        "                                                (original_image_np.shape[1], original_image_np.shape[0]),\n",
        "                                                interpolation=cv2.INTER_NEAREST)\n",
        "\n",
        "            predicted_color_mask = decode_segmentation_map(predicted_mask_resized)\n",
        "\n",
        "            # 원본 이미지와 예측된 마스크를 합성하여 시각화\n",
        "            blended_image = cv2.addWeighted(original_image_np, 0.7, predicted_color_mask, 0.3, 0) # 0.3은 마스크 투명도\n",
        "\n",
        "            plt.subplot(num_samples_to_show, 1, i + 1)\n",
        "            plt.imshow(blended_image)\n",
        "            plt.title(f\"[{image_name}] Inference Time: {inference_time:.3f}s\")\n",
        "            plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    print(\"--- 추론 및 시각화 완료 ---\")"
      ],
      "metadata": {
        "id": "k7bB1RoR2Pmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6 메인 실행 블록"
      ],
      "metadata": {
        "id": "yNvrrQTyxgCm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # 1. BDD10K 데이터셋 로드\n",
        "    val_dataset = BDD10KSegmentationDataset(split='val', transform=TRANSFORM_IMG, target_transform=TRANSFORM_MASK)\n",
        "    print(f\"BDD10K Validation 데이터셋 로드 완료: {len(val_dataset)}개 이미지\")\n",
        "\n",
        "    if not val_dataset:\n",
        "        print(\"로드된 BDD10K Segmentation 데이터가 없습니다. 경로 설정을 확인해주세요.\")\n",
        "    else:\n",
        "        # 2. UPerNet 모델 생성\n",
        "        # ResNet50 백본을 사용하고, num_classes는 위에서 정의한 3개 클래스 (배경, 주행 가능, 차선)\n",
        "        model = UPerNet(num_classes=NUM_CLASSES, backbone_name='resnet50', pretrained_backbone=False).to(device)\n",
        "        print(\"UPerNet 모델 생성 완료 (ResNet50 백본).\")\n",
        "\n",
        "        # 3. 학습 부분 (실행하지 않음)\n",
        "        # train_upernet_model(model, train_loader, val_loader)\n",
        "\n",
        "        # 4. 사전 학습 모델 로드 및 추론 실행\n",
        "        run_segmentation_inference(model, val_dataset, num_samples_to_show=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oEGKPeYpPHd",
        "outputId": "6470ca9a-742f-4e99-dfbb-94ba877efa60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BDD10K Validation 데이터셋 로드 완료: 0개 이미지\n",
            "로드된 BDD10K Segmentation 데이터가 없습니다. 경로 설정을 확인해주세요.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    # 'val' 디렉토리의 절대 경로 설정\n",
        "    val_image_directory = os.path.join(BDD10K_DATA_ROOT_PATH, \"val\")\n",
        "\n",
        "    # 'val' 디렉토리에서 JPG 이미지 파일 경로 목록만 가져옴\n",
        "    image_paths_for_inference = get_image_paths_from_dir(val_image_directory)\n",
        "    print(f\"BDD10K Validation 이미지 디렉토리에서 {len(image_paths_for_inference)}개 이미지 경로 로드 완료.\")\n",
        "\n",
        "    if not image_paths_for_inference:\n",
        "        print(\"로드된 BDD10K 이미지 경로가 없습니다. 'val' 디렉토리 또는 경로 설정을 확인해주세요.\")\n",
        "\n",
        "    else:\n",
        "        # UPerNet 모델 생성\n",
        "        model = UPerNet(num_classes=NUM_CLASSES, backbone_name='resnet50', pretrained_backbone=False).to(device)\n",
        "        print(\"UPerNet 모델 생성 완료 (ResNet50 백본).\")\n",
        "\n",
        "        run_segmentation_inference_on_random_samples(model, image_paths_for_inference, TRANSFORM_IMG, num_samples_to_show=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "t6zn4FLMpO9A",
        "outputId": "721f2e17-d1b8-4d2b-a139-acaa38c355e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BDD10K Validation 이미지 디렉토리에서 1000개 이미지 경로 로드 완료.\n",
            "UPerNet 모델 생성 완료 (ResNet50 백본).\n",
            "\n",
            "--- 사전 학습 모델 로드 및 랜덤 5개 샘플 추론 시작 ---\n",
            "'/content/pretrained/upernet_r50-d8_769x769_40k_sem_seg_bdd100k.pth' 모델 가중치를 성공적으로 로드했습니다.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Given groups=1, weight of size [128, 2048, 1, 1], expected input[1, 256, 1, 1] to have 2048 channels, but got 256 channels instead",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-501560192.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"UPerNet 모델 생성 완료 (ResNet50 백본).\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mrun_segmentation_inference_on_random_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_paths_for_inference\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTRANSFORM_IMG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples_to_show\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3574846581.py\u001b[0m in \u001b[0;36mrun_segmentation_inference_on_random_samples\u001b[0;34m(model, image_paths_list, preprocess_transform, num_samples_to_show)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;31m# 추론 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, num_classes, H, W)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0minference_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2605499731.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# C5 특징맵에 PPM 적용\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mppm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# c_features[-1]은 c5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0mppm_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppm_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mppm_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (B, ppm_out_channels, H_c5, W_c5)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-175514792.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_size\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'bilinear'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign_corners\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \"\"\"\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    541\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m             )\n\u001b[0;32m--> 543\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    544\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         )\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 2048, 1, 1], expected input[1, 256, 1, 1] to have 2048 channels, but got 256 channels instead"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2000x2500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "Tdo-_CphCfSv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}