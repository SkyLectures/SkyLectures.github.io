---
layout: page
title:  "대형 언어 모델(LLM) 개요"
date:   2025-04-01 09:00:00 +0900
permalink: /materials/S03-05-01-00_LLM_Overview
categories: materials
---

## 1. 대형 언어 모델(LLM)이란?
- 자연어 처리(Natural Language Processing, NLP) 분야의 핵심 기술
- 방대한 양의 텍스트 데이터를 학습하여 인간 언어를 이해하고 생성할 수 있는 인공지능(AI) 모델
- 딥러닝 기술을 기반으로 하며, 다양한 응용 분야에서 활용되고 있음


## 2. LLM의 주요 특징
- 방대한 데이터 학습
    - 인터넷 기사, 책, 논문 등 다양한 소스에서 수집된 대규모 텍스트 데이터를 학습
    - Common Crawl, Wikipedia와 같은 데이터 세트가 주로 사용됨

- 트랜스포머 아키텍처
    - 트랜스포머(Transformer) 신경망 구조를 사용
    - 셀프 어텐션(Self-Attention) 메커니즘을 통해 텍스트 간의 관계를 분석
    - 순차 처리가 아닌 병렬 처리를 통해 학습 속도를 크게 향상시킴

- 자연어 이해 및 생성
    - 문법, 구문, 의미적 관계를 학습하여 맥락에 맞는 응답을 생성
    - 사용자의 질문에 답변하거나 텍스트 요약, 번역 등을 수행할 수 있음

- 비지도 학습 및 미세 조정
    - 초기에는 비지도 학습으로 언어의 일반적인 패턴을 학습
    - 이후 특정 작업에 맞게 미세 조정(Fine Tuning)


## 3. LLM의 학습 과정
1. 데이터 수집 및 전처리
    - 다양한 소스에서 데이터를 수집하고 이를 정리 및 표준화
2. 토큰화
    - 텍스트를 작은 단위(토큰)로 나누어 처리
3. 모델 훈련
    - 딥러닝 알고리즘을 통해 데이터에서 패턴과 관계를 학습
4. 평가 및 최적화
    - 모델 성능을 평가하고 수정하여 정확도 향상
5. 배포
    - 실제 환경에서 사용할 수 있도록 배포


## 4. 주요 응용 분야
- 텍스트 생성
    - 에세이 작성, 이메일 초안 생성 등 다양한 형식의 텍스트 작성
- 번역 및 요약
    - 언어 간 번역 및 방대한 문서의 핵심 요약 가능
- 질문 응답 시스템
    - 사용자의 질문에 대해 정확하고 맥락적인 답변 제공
- 코드 생성 및 디버깅
    - 프로그래밍 코드 작성과 오류 수정에 활용


## 5. LLM의 장점과 한계
- 장점
    - 인간과 유사한 자연스러운 언어 생성
    - 다양한 작업에 적응 가능한 범용성
- 한계
    - 데이터 편향 문제
        - 훈련 데이터의 품질이 결과에 영향을 미침
    - 높은 계산 비용
        - 대규모 모델 훈련에는 막대한 자원이 필요함


## 6. LLM과 기존 AI 모델의 차이점

### 6.1 학습 데이터와 범용성
- 기존 AI 모델
    - 특정 작업(Task)에 맞게 설계되고 학습된 경우가 많음
        - 예: 텍스트 분류나 감정 분석과 같은 단일 목적의 작업을 수행함
    - 제한된 데이터셋을 사용
    - 새로운 작업을 수행하려면 별도의 재학습이 필요함
  
- LLM
    - 방대한 양의 텍스트 데이터를 학습하여 다양한 언어 기반 작업을 수행할 수 있는 범용성을 갖춤
    - 하나의 모델로 번역, 요약, 질문 응답 등 여러 작업을 처리할 수 있음


### 6.2 트랜스포머 기반 아키텍처
- 기존 AI 모델
    - RNN(Recurrent Neural Network) 또는 CNN(Convolutional Neural Network)과 같은 전통적 신경망 구조를 사용하는 경우가 많음
    - 이 구조는 긴 문맥의 처리에 한계를 가짐
  
- LLM
    - 트랜스포머(Transformer) 아키텍처를 기반으로 함
    - 셀프 어텐션(Self-Attention) 메커니즘을 통해 문맥을 효과적으로 이해
    - 이를 통해 더 자연스러운 언어 생성이 가능함


### 6.3 처리 능력과 응답 방식
- 기존 AI 모델
    - 규칙 기반 또는 제한된 데이터 패턴에 따라 동작
    - 예/아니오"와 같은 단순한 답변을 제공하는 경우가 많음
  
- LLM
    - 인간과 유사한 자연스러운 응답을 생성
    - 구조화되지 않은 질문에도 적절한 대응 가능
        - 예: 복잡한 문서 작성이나 창의적인 텍스트 생성 가능


### 6.4 사전 학습 및 전이 학습
- 기존 AI 모델
    - 특정 데이터셋에서 학습된 후 다른 작업에 활용하기 어려운 경우가 많음
  
- LLM
    - 사전 학습(Pre-training)을 통해 방대한 데이터를 학습
    - 전이 학습(Transfer Learning)을 통해 특정 도메인에 쉽게 적용 가능


### 6.5 계산 비용 및 효율성
- 기존 AI 모델
    - 상대적으로 적은 연산 자원으로 훈련 및 운영 가능
  
- LLM
    - 수십억~수천억 개의 파라미터를 포함함
    - 고성능 GPU와 대규모 분산 컴퓨팅 환경 필요
    - 높은 계산 비용이 주요 한계로 지적됨


## 7. LLM 개발의 역사: 주요 이슈와 시간 순서

- 대형 언어 모델(LLM)의 발전은 자연어 처리(NLP)와 인공지능(AI) 분야의 주요 혁신을 통해 이루어짐

- **1960~1990년대: 초기 NLP와 신경망의 등장**
    - 1966년: Eliza
        - MIT의 Joseph Weizenbaum이 개발한 최초의 챗봇
        - 패턴 매칭과 규칙 기반 시스템 사용
        - 심리치료사와 유사한 대화를 시뮬레이션함
        - 자연어 처리 연구의 시작으로 알려짐

    - 1980~1990년대: 신경망과 RNN
        - 신경망 기술이 발전하면서 데이터 학습과 패턴 인식이 가능해짐 
        - Recurrent Neural Networks(RNN)
            - 순차 데이터를 처리할 수 있는 능력 제공
            - 더 복잡한 언어 모델 개발의 기초 마련

- **1997~2010년: LSTM과 NLP 도구의 발전**
    - 1997년: Long Short-Term Memory(LSTM)
        - 긴 문맥을 처리할 수 있는 능력 제공
        - 이후 NLP 작업에 중요한 역할을 함

    - 2010년: Stanford CoreNLP
        - 감정 분석, 개체명 인식 등 복잡한 NLP 작업을 처리할 수 있는 도구 세트 제공
        - 자연어 처리에 대한 연구의 가속화

- **2011~2017년: 워드 임베딩과 트랜스포머**
    - 2013년: Word2Vec
        - Google의 Tomas Mikolov 팀이 워드 임베딩 기술을 도입하여 개발
        - 단어 간 의미적 관계를 효율적으로 학습할 수 있게 함

    - 2017년: 트랜스포머 아키텍처
        - Google Brain이 발표한 "Attention is All You Need" 논문에서 트랜스포머 모델을 소개
        - LLM 개발의 전환점 마련
        - 셀프 어텐션 메커니즘을 통해 대규모 데이터 학습이 가능해 짐

- **2018~2020년: GPT 시리즈와 BERT**
    - 2018년: GPT(OpenAI)와 BERT(Google)
        - 6월: GPT 발표(OpenAI)
            - 트랜스포머 아키텍처 기반
            - 자연어 처리 분야에서 새로운 가능성을 보여줌
            - 대형 언어 모델 개발의 기초를 마련함
                - 1.17억개 파라미터
        - 10월31일: BERT 발표(Google)
            - 양방향 텍스트 이해를 가능하게 함
            - NLP 작업의 성능을 크게 향상시킴

    - 2019~2020년: GPT-2와 GPT-3
        - 2019년: GPT-2(15억개 파라미터) 발표
            - 인간과 유사한 텍스트 생성 능력을 보여줌
        - 2020년: GPT-3(1,750억개 파라미터) 발표
            - 언어 모델로서 거의 모든 NLP 작업을 다룰 수 있음
            - 번역, 질문 응답 등 다양한 작업에서 뛰어난 성능 발휘
            - 일반 사용자가 API를 통해 접근할 수 있는 형태로 출시됨 (OpenAI API)

- **2021~2023년: 멀티모달 모델과 대중화**
    - 2021년: LaMDA와 CLIP
        - LaMDA(Google)
            - 대화형 AI에 특화된 모델로 개발됨
        - CLIP(OpenAI)
            - 이미지와 텍스트를 연결하는 멀티모달 모델로 주목

    - 2022년: ChatGPT 출시(OpenAI)
        - OpenAI가 GPT-3.5 기반 ChatGPT 출시(11월30일)
        - 대중에게 LLM 기술을 소개
        - 상호작용형 AI 응용 프로그램의 가능성을 보여줌

    - 2023년:
        - 3월: GPT-4 출시(OpenAI)
            - 이전 모델(GPT-3.5) 대비 약 500배 더 큰 데이터셋 활용
            - 멀티모달 기능 도입
        - 11월: GPT-4 Turbo 출시(OpenAI)
            - GPT-4의 경량화 버전
            - 비슷한 성능에 더 빠르고 비용 효율적인 처리 가능

- **2024~2025년: 모델 성능 및 비용 효율화**
    - 2024년: 
        - GPT-4o(OpenAI) 발표(5월13일)
            - 텍스트와 이미지 외에도 비디오 등 다양한 형태의 멀티모달 데이터를 처리하는 기능 추가
            - 언어 이해 및 생성 능력을 더욱 강화
    - 2025년:
        - DeepSeek(Hangzhou DeepSeek AI 연구소) 발표(1월10일)
            - Mixture-of-Experts(MoE) 아키텍처를 기반으로 설계
            - 효율적인 연산과 높은 성능을 제공함
            - 논리적 추론, 복잡한 문제 해결, 멀티모달 학습(텍스트, 이미지, 오디오 데이터 처리) 지원
            - 2024년 5월, V2가 소개되었으며 공식 발표는 V3 버전임